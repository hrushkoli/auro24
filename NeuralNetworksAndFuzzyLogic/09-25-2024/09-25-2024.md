# 09-25-2024 NNFL

## Difference between Stochaistic Gradient Descent and Mini Batch Gradient Descent
- In Stochaistic Gradient Descent, each record is trained one by one.
- In Mini Batch Gradient Descent, records are trained in batches.

```python
import tensorflow as tf 
from tensorflow.keras import layers, models 
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split 
import pandas as pd 
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import boston_housing

## load the data 
(train_data,train_targets),(test_data,test_targets) = boston_housing.load_data()

print("Train Data: ", len(train_data))
print("Test Data: ", len(test_data))

column_names = ['CRIM','ZN', 'INDUSA','CHAS','NOX', 'RM','AGE','DIS','RAD','TAX','PIRATIO','B','LSTAT','MEDV'
]

# convert training data to a df 
train_df = pd.DataFrame(train_data, columns = column_names[:-1])
train_df['MEDV'] = train_targets

#display first few rows
print(train_df)

## normalizing the data
scaler = StandardScaler()
train_data = scaler.fit_transform(train_data)
test_data = scaler.transform(test_data)

train_data, val_data, train_targets, val_targets = train_test_split(train_data, train_targets, test_size = 0.2, random_state=42)

model = models.Sequential()
model.add(layers.Dense(64,activation='relu',input_shape=(train_data.shape[1],)))
model.add(layers.Dense(64,activation='relu'))
model.add(layers.Dense(1)) # output layer for regression
model.summary()

## compile the model.

model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])

history = model.fit(train_data,train_targets,epochs = 100,batch_size=32,validation_data=(val_data,val_targets))

test_mse_score, test_mae_score = model.evaluate(test_data,test_targets)
print(f'Test MSE: {test_mse_score}')
print(f'Test MAE: {test_mae_score}')
# Plotting the training and validation loss\n
plt.figure(figsize=(12, 5))

# Plot loss vs val_loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')

# Plot mae vs val_mae\n",
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Aaccuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.tight_layout()
plt.show()



import numpy as np
# Assuming `scaler` and `model` are already defined and trained
# Example new data point (you should replace this with your actual data)
new_data_point = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0,1.1, 1.2, 1.3])  # Should have the same number of features as your training data
# Normalize the new data point using the same scaler\n",
new_data_point_scaled = scaler.transform(new_data_point.reshape(1, -1))
# Make prediction\n",
predicted_price = model.predict(new_data_point_scaled)
print(f"Predicted Price: {predicted_price[0][0]:.2f}")


model.save('house_price_prediction_model.keras')
```
