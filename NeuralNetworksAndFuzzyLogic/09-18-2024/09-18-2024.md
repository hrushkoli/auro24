# 09-18-2024 NNFL - Neural Network Phases

## Pipeline of Neural Networks

1. Problem Statement Analysis
2. Data Collection 
    Can be done through Web Scraping or other methods.
3. Data Preparation & Preprocessing
4. Feature Engineering (Correlation Matrix, Chi Square Matrix, etc)
5. Exploratory Data Analysis
6. Data Splitting
8. Model Selection
8. Model Training
9. Model Evaluation
10. Model on Prod

## Various Kinds of Activation Functions

### Sigmoid Function

- Gives values in range 0-1

### SoftMax Function

- Based on the number of classes
- It calculates the probability and assigns weightage to a class accordingly

<img title="Sigmoid vs SoftMax" src="D:\Hrishi\Auro24\NeuralNetworksAndFuzzyLogic\09-18-2024\images\sigmoid vs softmax.png"/>

### Tanh Function
- Used for continuous data.
- Used for scaling purposes in hidden layers of NN
- Ranged from -1 to 1
<img title="tanh" src="D:\Hrishi\Auro24\NeuralNetworksAndFuzzyLogic\09-18-2024\images\tanh.ppm"/>

### ReLu Function
- Used for continuous data.
- Ranges from 0 to 1
<img title="ReLu" src="D:\Hrishi\Auro24\NeuralNetworksAndFuzzyLogic\09-18-2024\images\relu graph.png"/>

### Leaky Relu function
- Used for continuous data.
- Ranges from -1 to 1
<img title="Leaky ReLu" src="D:\Hrishi\Auro24\NeuralNetworksAndFuzzyLogic\09-18-2024\images\Leaky relu.png"/>

## Back Propogation

An alogorithm for compiting the gradient of the loss function with respect to each weight by applying the chain rule of calculus. This information is used to update the weights
Find weights and biaes by finding the local minima using optimizers (Gradient Descent,etc.)

### Gradient Descent:
An optimization algorithm that updates the network's weights to minimize the loss function. Its variants include Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, and Adam.

## Example of Training a Neural Network for handwritten Digit Recognition.

1. Dataset
2. Initialization: Randomly initialize weights and biases
3. Forward Propogation : Pass each image through the network to obtain a predicted digit.
4. loss Calculation: Compute the Cross-Entropy loss between the predicted digit and actual digit.
5. Backpropogation: Calculate the gradient of the loss wrt each weight and biases
6. Weight Update: Use gradient descent to adjust the weights and biases, reducing the loss
7. Iteration


```python
import tensorflow as tf
import numpy as np
from tensorflow import keras

def demo_model(y_new):
    xs = np.array([1,2,3,4,5,6])
    ys = np.array([1.0,1.5,2.0,2.5,3.0,3.5])
    model = tf.keras.Sequential([keras.layers.Dense(units=1,input_shape=[1])])
    model.compile(optimizer='sgd',loss='mean_squared_error')
    model.fit(xs,ys,epochs=500)
    return model.predict(np.array(y_new))[0]

prediction = demo_model([7.0])
print(prediction)
```

